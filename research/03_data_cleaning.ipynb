{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roshi\\AppData\\Local\\Temp\\ipykernel_3560\\1840574479.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from urllib import request\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\roshi\\\\OneDrive\\\\Desktop\\\\Git-1\\\\Text_Classification_ML_NLP'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataCleaningConfig:\n",
    "    root_dir         : Path\n",
    "    data_dir         : Path\n",
    "    new_data_dir     : Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.text_Classification_ML.utils.common import *\n",
    "from src.text_Classification_ML.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_file_path = Config_File_Path,\n",
    "            schema_file_path = Schema_File_Path\n",
    "            ):\n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.schema = read_yaml(schema_file_path)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_cleaning_config(self) -> DataCleaningConfig:\n",
    "        config = self.config.data_cleaning\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_cleaning_config = DataCleaningConfig(\n",
    "            root_dir= config.root_dir,\n",
    "            data_dir= config.data_dir,\n",
    "            new_data_dir= config.new_data_dir,\n",
    "        )\n",
    "\n",
    "        return data_cleaning_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import contractions\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaning:\n",
    "    def __init__(self, config: DataCleaningConfig):\n",
    "        self.config = config\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.df = pd.read_csv(self.config.data_dir)\n",
    "    \n",
    "    def clean_text(self):\n",
    "        self.df['Cleaned_Review'] = self.df['Review'].apply(self._clean_single_text)\n",
    "        return self.df\n",
    "\n",
    "\n",
    "    def _clean_single_text(self, text):\n",
    "        text = emoji.demojize(text)  # fix emojis\n",
    "        text = contractions.fix(text)  # fix contractions\n",
    "        text = re.sub(r'[^\\x00-\\x7f]', r'', text)  # remove strange fonts\n",
    "        text = re.sub(r\"\\d+\", \"number\", text)  # replace numbers with \"number\"\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # remove non-alphanumeric chars\n",
    "        text = text.replace('_', ' ')  # replace underscores with space\n",
    "        text = re.sub(r'[^A-Z a-z 0-9-]+', '', text)\n",
    "        text = text.strip()  # strip extra spaces\n",
    "        text = text.lower()\n",
    "        text = self._remove_accented_chars(text)  # remove accented characters\n",
    "        text = self._remove_stop_words(text)  # remove stop words\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "    def _remove_accented_chars(self, text):\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        return text\n",
    "\n",
    "\n",
    "    def _remove_stop_words(self, text):\n",
    "        text_tokens = word_tokenize(text)\n",
    "        filtered_tokens = [word for word in text_tokens if word.lower() not in self.stop_words]\n",
    "        filtered_text = ' '.join(filtered_tokens)\n",
    "        return filtered_text\n",
    "    \n",
    "\n",
    "    def drop_columns_and_duplicates(self):\n",
    "        self.df.drop(columns=['Time_submitted','Total_thumbsup','Reply','Review'],inplace=True, axis=1)\n",
    "        return self.df\n",
    "    \n",
    "\n",
    "    def word_lemitization(self):\n",
    "        self.df['Cleaned_Review'] = self.df['Cleaned_Review'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "        return self.df\n",
    "    \n",
    "    \n",
    "    def save_to_csv(self, filename= \"data_spotify_new.csv\"):\n",
    "        filepath = os.path.join(self.config.new_data_dir, filename)\n",
    "        self.df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_cleaning_config = config.get_data_cleaning_config()\n",
    "    data_cleaning = DataCleaning(config=data_cleaning_config)\n",
    "    data_cleaning.clean_text()\n",
    "    data_cleaning.drop_columns_and_duplicates()\n",
    "    data_cleaning.word_lemitization()\n",
    "    data_cleaning.save_to_csv()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
